{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python 3.7 built-in\n",
    "import datetime\n",
    "import itertools\n",
    "import multiprocessing\n",
    "import operator\n",
    "import os\n",
    "import pathlib\n",
    "import pandas\n",
    "import random\n",
    "import re\n",
    "import shutil\n",
    "import subprocess\n",
    "import time\n",
    "import uuid\n",
    "\n",
    "# additional dependencies\n",
    "from Bio import SeqIO  # pip3 install biopython\n",
    "import tqdm            # pip3 install tqdm\n",
    "import xlsxwriter      # pip3 install xslxwriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameter configuration\n",
    "results_directory = 'results'\n",
    "\n",
    "experiment_name = 'N1_Normal'\n",
    "all_denovo_file = 'data/N1_Normal/N1_Normal_all_de_novo.csv'\n",
    "db_matched_file = 'data/N1_Normal/N1_Normal_DB-matched_peptides.csv'\n",
    "reference_proteome_file = 'data/SPHu.fasta'\n",
    "\n",
    "min_peptide_length = 7\n",
    "max_peptide_length = 25\n",
    "alc_score_cutoff = 50\n",
    "max_fusion_distance = 40\n",
    "number_of_cores = multiprocessing.cpu_count()\n",
    "\n",
    "include_PTMs = True     # if True, converts C(+57.02)FKHSGTGM(+15.99)VHR to CFKHSGTGMVHR; else, skips the row\n",
    "\n",
    "# create working directory if it does not exist; all output will be stored here\n",
    "results_path = results_directory + os.sep + experiment_name\n",
    "pathlib.Path(results_path).mkdir(parents=True, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read data\n",
    "df_db_matched = pandas.read_csv(db_matched_file, dtype={'Scan': int})\n",
    "df_all_denovo = pandas.read_csv(all_denovo_file, dtype={'ALC (\\%)': int, 'Scan': int})\n",
    "ref_db = {seq.name: str(seq.seq) for seq in SeqIO.parse(reference_proteome_file, 'fasta')}\n",
    "\n",
    "# process PTMs\n",
    "if include_PTMs:\n",
    "    df_db_matched['Peptide'] = df_db_matched['Peptide'].apply(lambda x: re.sub(r'\\([^()]*\\)', '', x))\n",
    "    df_all_denovo['Peptide'] = df_all_denovo['Peptide'].apply(lambda x: re.sub(r'\\([^()]*\\)', '', x))\n",
    "else:\n",
    "    df_db_matched = df_db_matched[~df_db_matched['Peptide'].str.contains('\\+')]\n",
    "    df_all_denovo = df_all_denovo[~df_all_denovo['Peptide'].str.contains('\\+')]\n",
    "\n",
    "# save DB-matched file with razor peptides removed\n",
    "df_db_matched_no_razor_peptides = df_db_matched[df_db_matched['Accession'].apply(lambda x: str(x).count(':')) == 0]\n",
    "df_db_matched_no_razor_peptides.to_csv(results_path + os.sep + 'DB-matched_no_razor_peptides.csv', index=False)\n",
    "\n",
    "# save the unique protein IDs with razor peptides removed\n",
    "# TODO: clarify which entries to keep - i.e. do we just keep the first occurence of each duplicate protein?\n",
    "df_db_matched_no_razor_peptides_unique = df_db_matched_no_razor_peptides.drop_duplicates(subset='Accession',\n",
    "                                                                                         keep='first')\n",
    "df_db_matched_no_razor_peptides_unique.to_csv(results_path + os.sep + 'DB-matched_no_razor_peptides_unique_proteinIDs.csv', index=False)\n",
    "\n",
    "# get all denovo scan IDs that are not found in DB-matched data\n",
    "scan_ids_not_db_matched = set(df_all_denovo['Scan'].values) - set(df_db_matched['Scan'].values)\n",
    "df_all_denovo_unmatched = df_all_denovo[df_all_denovo['Scan'].isin(scan_ids_not_db_matched)]\n",
    "df_all_denovo_unmatched.to_csv(results_path + os.sep + 'Fused_peptide_candidates.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# classes to structure the Peptides data for scanning purposes\n",
    "class Peptide:\n",
    "    def __init__(self, name, scanID, score):\n",
    "        self.name = name\n",
    "        self.scanID = scanID\n",
    "        self.score = score\n",
    "        self.unique_identifier = str(uuid.uuid4())\n",
    "        self.fragments = []\n",
    "\n",
    "\n",
    "class FragmentPair:\n",
    "    def __init__(self, pair):\n",
    "        self.pair = pair\n",
    "        self.matchPairs = []\n",
    "\n",
    "\n",
    "class Match:\n",
    "    def __init__(self, name, start, stop, number):\n",
    "        self.name = name\n",
    "        self.start = start\n",
    "        self.stop = stop\n",
    "        self.ORF = number\n",
    "\n",
    "\n",
    "class FusedPeptide:\n",
    "    def __init__(self):\n",
    "        self.name = None\n",
    "        \n",
    "        self.fragment_1 = None\n",
    "        self.fragment_1_start = None\n",
    "        self.fragment_1_stop = None\n",
    "        self.match_1_name = None\n",
    "        self.match_1_start = None\n",
    "        self.match_1_stop = None\n",
    "        \n",
    "        self.fragment_2 = None\n",
    "        self.fragment_2_start = None\n",
    "        self.fragment_2_stop = None\n",
    "        self.match_2_name = None\n",
    "        self.match_2_start = None\n",
    "        self.match_2_stop = None\n",
    "        \n",
    "        self.cis_distance_12 = 'N/A'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# methods for peptide fusion analysis and running\n",
    "def _create_combinations_with_isoleucines(peptide):\n",
    "    \"\"\"\n",
    "    Performs all possible combinations of string reassignment of L to I.\n",
    "    \"\"\"\n",
    "    count = peptide.count('L')\n",
    "    product = [''.join(seq) for seq in itertools.product(\"01\", repeat=count)]\n",
    "    positions = [pos for pos, char in enumerate(peptide) if char == 'L']\n",
    "    peptides = []\n",
    "    for combo in product:\n",
    "        partitioned_peptide = list(peptide)\n",
    "        for pos, item in enumerate(positions):\n",
    "            if combo[pos] == '1':\n",
    "                partitioned_peptide[item] = 'I'\n",
    "        peptides.append(''.join(partitioned_peptide))\n",
    "    return peptides\n",
    "\n",
    "\n",
    "def _fragment_peptide(peptide):\n",
    "    \"\"\"\n",
    "    Fragments the provided peptide according to the length requirement.\n",
    "    \n",
    "    Returns a list of fragment pairs, in order, sorted by full length peptide fragment first.\n",
    "    \"\"\"\n",
    "    fragments = []\n",
    "    \n",
    "    # generate I/L variants\n",
    "    IL_variants = _create_combinations_with_isoleucines(peptide.name)\n",
    "    for fragment in IL_variants:\n",
    "        fragments.append(FragmentPair(['', fragment]))\n",
    "    \n",
    "    # generate spliced peptide fragments\n",
    "    for IL_variant in IL_variants:\n",
    "        for position in range(2, len(IL_variant)-1):\n",
    "            fragments.append(FragmentPair([IL_variant[:position], IL_variant[position:]]))\n",
    "    \n",
    "    # sort fragments, longest fragments (full length) first - necessary for excluding them from splice searches if they match database in full\n",
    "    fragments.sort(key=lambda x: len(x.pair[-1]), reverse=True)\n",
    "    \n",
    "    return fragments\n",
    "\n",
    "\n",
    "def _search_for_fragment_in_protein_by_splitting_ORFs(peptide, references_data, max_search_distance=40):\n",
    "    \"\"\"\n",
    "    Creates the reference search space for the sequence. Updates the Peptide\n",
    "    object's Fragment objects with any Match objects it finds.\n",
    "    \n",
    "    First fragment to be searched is full length peptide itself - which means if matched,\n",
    "    the search will stop before next fragment set gets a chance to be matched.\n",
    "    \n",
    "    Returns the updated Peptide.\n",
    "    \"\"\"\n",
    "    full_length_match_found = False\n",
    "    for fragment in peptide.fragments:\n",
    "        # stop looking for matches if full length fragment match found\n",
    "        if full_length_match_found:\n",
    "            break\n",
    "        \n",
    "        for protein_name, sequence in references_data.items():\n",
    "            sequence = sequence.strip('X*')\n",
    "            \n",
    "            distance = 0\n",
    "            if 'X' in sequence:\n",
    "                splitter = 'X'\n",
    "            else:\n",
    "                splitter = '*'\n",
    "            for number, ORF in enumerate(sequence.split(splitter)):\n",
    "                \n",
    "                # determine the side which has the big fragment (defined in fragment_peptide method)\n",
    "                if len(fragment.pair[1]) < len(fragment.pair[0]):\n",
    "                    large_fragment = fragment.pair[0]\n",
    "                    small_fragment = fragment.pair[1]\n",
    "                else:\n",
    "                    large_fragment = fragment.pair[1]\n",
    "                    small_fragment = fragment.pair[0]\n",
    "                \n",
    "                for match_data_large in re.finditer(large_fragment, ORF):\n",
    "                    \n",
    "                    match_large = Match(protein_name, match_data_large.start()+distance, match_data_large.end()-1+distance, number)\n",
    "                    \n",
    "                    # determine the search space start for small fragment matching\n",
    "                    starting_position = match_data_large.start()-(max_search_distance+len(small_fragment))\n",
    "                    if starting_position < 0:\n",
    "                        starting_position = 0\n",
    "                    \n",
    "                    # full length matches\n",
    "                    if len(small_fragment) == 0:\n",
    "                        full_length_match_found = True\n",
    "                        fragment.matchPairs.append(['N/A', match_large])\n",
    "                        continue\n",
    "                    \n",
    "                    for match_data_small in re.finditer(small_fragment, ORF[starting_position:match_data_large.end()+1+max_search_distance+len(small_fragment)]):\n",
    "                        \n",
    "                        match_small = Match(protein_name, starting_position+match_data_small.start()+distance, starting_position+match_data_small.end()-1+distance, number)\n",
    "                        \n",
    "                        # discard results if large fragment match overlaps with small fragment match\n",
    "                        if match_small.stop >= match_large.start and match_small.start <= match_large.stop:\n",
    "                            continue\n",
    "                        \n",
    "                        if len(fragment.pair[1]) < len(fragment.pair[0]):\n",
    "                            fragment.matchPairs.append([match_large, match_small])\n",
    "                        else:\n",
    "                            fragment.matchPairs.append([match_small, match_large])\n",
    "                    \n",
    "                distance += len(ORF)+1\n",
    "    return peptide\n",
    "\n",
    "\n",
    "def create_fused_peptide(f, fragment_1, fragment_2, match_1, match_2, peptide):\n",
    "    \"\"\"\n",
    "    Writes out fused peptide info.\n",
    "    \"\"\"\n",
    "    # create fused peptide info\n",
    "    fused_peptide = FusedPeptide()\n",
    "    fused_peptide.name = peptide.name\n",
    "    \n",
    "    # fused peptide match\n",
    "    if fragment_1:\n",
    "        fused_peptide.fragment_1 = fragment_1\n",
    "        fused_peptide.match_1_name = match_1.name\n",
    "        fused_peptide.match_1_start = match_1.start\n",
    "        fused_peptide.match_1_stop = match_1.stop\n",
    "        \n",
    "        fused_peptide.fragment_2 = fragment_2\n",
    "        fused_peptide.match_2_name = match_2.name\n",
    "        fused_peptide.match_2_start = match_2.start\n",
    "        fused_peptide.match_2_stop = match_2.stop\n",
    "        \n",
    "        if match_1.stop < match_2.start:\n",
    "            if match_1.stop+1 == match_2.start:\n",
    "                return 0    # skip output if fusion is not sensible\n",
    "            fused_peptide.cis_distance_12 = match_2.start - match_1.stop - 1\n",
    "        else:\n",
    "            fused_peptide.cis_distance_12 = match_1.start - match_2.stop - 1\n",
    "    \n",
    "    # full length peptide match\n",
    "    else:\n",
    "        fused_peptide.fragment_1 = fragment_2\n",
    "        fused_peptide.match_1_name = match_2.name\n",
    "        fused_peptide.match_1_start = match_2.start\n",
    "        fused_peptide.match_1_stop = match_2.stop\n",
    "        fused_peptide.cis_distance_12 = 'N/A'\n",
    "        \n",
    "    write_temp_file(f, fused_peptide, peptide)\n",
    "\n",
    "\n",
    "def write_temp_file(f, fused_peptide, peptide, unmatched=False):\n",
    "    \"\"\"\n",
    "    Write a temporary text file for later.\n",
    "    \"\"\"\n",
    "    if unmatched:\n",
    "        f.write('%s,%d,%s,%s,%s,%s,%s,%s,%s,%s,%d,%d,%s,%s\\n' %\n",
    "                (fused_peptide,\n",
    "                len(fused_peptide),\n",
    "                'N/A',\n",
    "                'N/A',\n",
    "                'N/A',\n",
    "                'N/A',\n",
    "                'N/A',\n",
    "                'N/A',\n",
    "                'N/A',\n",
    "                'N/A',\n",
    "                peptide.scanID,\n",
    "                peptide.score,\n",
    "                'de novo non-matched',\n",
    "                peptide.unique_identifier))\n",
    "        return\n",
    "    \n",
    "    if fused_peptide.cis_distance_12 != 'N/A':\n",
    "        f.write('%s,%d,%s,%s,%d,%d,%s,%s,%d,%d,%d,%d,%s,%s\\n' %\n",
    "                (fused_peptide.name,\n",
    "                len(fused_peptide.name),\n",
    "                fused_peptide.match_1_name,\n",
    "                fused_peptide.fragment_1,\n",
    "                fused_peptide.match_1_start+1,\n",
    "                fused_peptide.match_1_stop+1,\n",
    "                str(fused_peptide.cis_distance_12),\n",
    "                fused_peptide.fragment_2,\n",
    "                fused_peptide.match_2_start+1,\n",
    "                fused_peptide.match_2_stop+1,\n",
    "                peptide.scanID,\n",
    "                peptide.score,\n",
    "                'de novo matched and fused',\n",
    "                peptide.unique_identifier))\n",
    "    else:\n",
    "        f.write('%s,%d,%s,%s,%d,%d,%s,%s,%s,%s,%d,%d,%s,%s\\n' %\n",
    "                (fused_peptide.name,\n",
    "                len(fused_peptide.name),\n",
    "                fused_peptide.match_1_name,\n",
    "                fused_peptide.fragment_1,\n",
    "                fused_peptide.match_1_start+1,\n",
    "                fused_peptide.match_1_stop+1,\n",
    "                'N/A',\n",
    "                'N/A',\n",
    "                'N/A',\n",
    "                'N/A',\n",
    "                peptide.scanID,\n",
    "                peptide.score,\n",
    "                'de novo DB-matched',\n",
    "                peptide.unique_identifier))\n",
    "\n",
    "        \n",
    "def find_peptide_fusions(experiment, peptide, n_peptide, directory, references_data, max_search_distance):\n",
    "    \"\"\"\n",
    "    Finds peptides that could be fused and outputs their matches.\n",
    "    \"\"\"\n",
    "    peptide.fragments = _fragment_peptide(peptide)\n",
    "    \n",
    "    # find fragments in experimental data\n",
    "    peptide = _search_for_fragment_in_protein_by_splitting_ORFs(peptide, references_data, max_search_distance)\n",
    "    \n",
    "    f_name = '%s%s_%d.txt' % (directory, experiment, n_peptide)\n",
    "    f = open(f_name, 'w')\n",
    "    \n",
    "    # outputting the fusions\n",
    "    matched = False\n",
    "    for fragment in peptide.fragments:\n",
    "        for match_pair in fragment.matchPairs:\n",
    "            matched = True\n",
    "            create_fused_peptide(f, fragment.pair[0], fragment.pair[1], match_pair[0], match_pair[1], peptide)\n",
    "    \n",
    "    # store the peptides that were not matched as fused or to the reference proteome\n",
    "    if not matched:\n",
    "        write_temp_file(f, peptide.name, peptide, unmatched=True)\n",
    "    f.close()\n",
    "\n",
    "\n",
    "def running_python_jobs_count():\n",
    "    return len([i for i in subprocess.check_output(['ps', 'uax']).splitlines() if 'python' in str(i) and 'R' in str(i)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in peptide fusion candidates and eliminate duplicate rows\n",
    "df_fused_peptide_candidates = pandas.read_csv(results_path + os.sep + 'Fused_peptide_candidates.csv',\n",
    "                                              usecols=['Peptide', 'Scan', 'ALC (%)']).drop_duplicates()\n",
    "\n",
    "# generate peptide datastructure\n",
    "fused_peptide_candidates = [Peptide(*data[1].values) for data in df_fused_peptide_candidates.iterrows()]\n",
    "fused_peptide_candidates = fused_peptide_candidates[:1000] # TESTING: limiting here for test purposes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/ipykernel_launcher.py:18: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7f9114b5e0cc4df892c97668855256ce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=1000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# perform peptide fusion; if recent files exist, prevent override unless multithreaded_temp_files are deleted\n",
    "temp_files_path = results_directory + os.sep + experiment_name + os.sep + 'multithreaded_temp_files' + os.sep\n",
    "pathlib.Path(temp_files_path).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# setup jobs\n",
    "jobs = []\n",
    "for n_peptide, peptide in enumerate(fused_peptide_candidates):\n",
    "    job = multiprocessing.Process(target=find_peptide_fusions,\n",
    "                                  args=(experiment_name,\n",
    "                                        peptide,\n",
    "                                        n_peptide,\n",
    "                                        temp_files_path,\n",
    "                                        ref_db,\n",
    "                                        max_fusion_distance))\n",
    "    jobs.append(job)\n",
    "\n",
    "# run jobs\n",
    "progress_bar = tqdm.tqdm_notebook(total=len(jobs))\n",
    "while len(jobs) > 0:\n",
    "    if running_python_jobs_count() <= number_of_cores:\n",
    "        job = jobs.pop()\n",
    "        job.start()\n",
    "        progress_bar.update(1)\n",
    "progress_bar.close() # Note: this will complete just before the final peptides have finished scanning\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assemble all results into a single file\n",
    "header = ['Peptide', 'Length', 'ProteinID',\n",
    "          'Fragment 1', 'Match 1 Start', 'Match 1 Stop', 'Cis Distance',\n",
    "          'Fragment 2', 'Match 2 Start', 'Match 2 Stop',\n",
    "          'Scan', 'ALCScore', 'Origin', 'ParentID']\n",
    "\n",
    "df = pandas.DataFrame(columns=header)\n",
    "for f in os.listdir(temp_files_path):\n",
    "    df = pandas.concat([df, pandas.read_csv(temp_files_path + os.sep + f, names=header)], axis=0)\n",
    "\n",
    "try:\n",
    "    shutil.rmtree(temp_files_path)\n",
    "except:\n",
    "    print('Directory already removed or no permissions to delete.')\n",
    "\n",
    "df = df.fillna('N/A')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_direction(start, end, origin):\n",
    "    if origin == 'de novo matched and fused':\n",
    "        if start < end:\n",
    "            return 'Forward'\n",
    "        else:\n",
    "            return 'Reverse'\n",
    "    else:\n",
    "        return 'N/A'\n",
    "\n",
    "df['Direction'] = df.apply(lambda row: get_direction(row['Match 1 Start'], row['Match 2 Start'], row['Origin']), axis=1)\n",
    "df.to_csv(results_path + os.sep + 'Matched_all.csv', index=False)\n",
    "df[df['Direction'] == 'Forward'].to_csv(results_path + os.sep + 'Matched_denovo_Fused_Forward.csv', index=False)\n",
    "df[df['Direction'] == 'Reverse'].to_csv(results_path + os.sep + 'Matched_denovo_Fused_Reverse.csv', index=False)\n",
    "df[df['Origin'] == 'de novo DB-matched'].to_csv(results_path + os.sep + 'Matched_denovo_DB-matched.csv', index=False)\n",
    "df[df['Origin'] == 'de novo non-matched'].to_csv(results_path + os.sep + 'Unmatched_denovo.csv', index=False)\n",
    "\n",
    "df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.Origin.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
